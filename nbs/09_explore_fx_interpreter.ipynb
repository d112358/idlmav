{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook uses the `Interpreter` class from the `torch.fx` library to extract shape information from nodes after symbolic tracing has been performed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.fx as fx\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from tabulate import tabulate\n",
    "import torchprofile\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-written CNN (for MNIST)\n",
    "* Found [here](https://github.com/pytorch/examples/blob/main/mnist/main.py)\n",
    "* Modified for experimental purposes, e.g. breaking the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "def get_model_cnn_mnist():\n",
    "    return MnistCnn()\n",
    "\n",
    "def get_model_cnn_mnist_broken():\n",
    "    model = MnistCnn()\n",
    "    model.fc1 = nn.Linear(9216, 120)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18 (from TorchVision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_resnet18():\n",
    "    return torchvision.models.resnet.resnet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_input_size(name, device):\n",
    "    match name:\n",
    "        case 'cnn_mnist':\n",
    "            return get_model_cnn_mnist().to(device), (16,1,28,28)\n",
    "        case 'cnn_mnist_broken':\n",
    "            return get_model_cnn_mnist_broken().to(device), (16,1,28,28)\n",
    "        case 'resnet18':\n",
    "            return get_model_resnet18().to(device), (16,3,160,160)\n",
    "        case _:\n",
    "            return None, None\n",
    "        \n",
    "def get_model_and_input_tensor(name, device):\n",
    "    model, input_size = get_model_and_input_size(name, device)\n",
    "    xb = torch.randn(input_size).to(device)\n",
    "    return model, xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model_names = ['cnn_mnist', 'resnet18', 'cnn_mnist_broken']\n",
    "model0, x0 = get_model_and_input_tensor(model_names[0], device)\n",
    "model1, x1 = get_model_and_input_tensor(model_names[1], device)\n",
    "model2, x2 = get_model_and_input_tensor(model_names[2], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeInterpreter(fx.Interpreter):\n",
    "    def __init__(self, mod : torch.nn.Module):\n",
    "        gm = fx.symbolic_trace(mod)\n",
    "        super().__init__(gm)\n",
    "\n",
    "        self.cur_macs: int = None\n",
    "        self.shapes : Dict[fx.Node, Tuple[int]] = {}\n",
    "        self.macs : Dict[fx.Node, int] = {}\n",
    "        self.param_counts : Dict[fx.Node, int] = {}\n",
    "        self.type_names : Dict[fx.Node,str] = {}\n",
    "\n",
    "    def rgetattr(self, m: nn.Module, attr: str) -> Tensor | None:\n",
    "        # From torchinfo, used in `get_param_count()`:\n",
    "        for attr_i in attr.split(\".\"):\n",
    "            if not hasattr(m, attr_i):\n",
    "                return None\n",
    "            m = getattr(m, attr_i)\n",
    "        assert isinstance(m, Tensor)  # type: ignore[unreachable]\n",
    "        return m  # type: ignore[unreachable]\n",
    "\n",
    "    def get_num_trainable_params(self, m:nn.Module):\n",
    "        num_params = 0\n",
    "        for name, param in m.named_parameters():\n",
    "            # We're only looking for trainable parameters here\n",
    "            if not param.requires_grad: continue\n",
    "\n",
    "            num_params_loop = param.nelement()\n",
    "\n",
    "            # From torchinfo `get_param_count()`:\n",
    "            # Masked models save parameters with the suffix \"_orig\" added.\n",
    "            # They have a buffer ending with \"_mask\" which has only 0s and 1s.\n",
    "            # If a mask exists, the sum of 1s in mask is number of params.\n",
    "            if name.endswith(\"_orig\"):\n",
    "                without_suffix = name[:-5]\n",
    "                pruned_weights = self.rgetattr(m, f\"{without_suffix}_mask\")\n",
    "                if pruned_weights is not None:\n",
    "                    num_params_loop = int(torch.sum(pruned_weights))\n",
    "            \n",
    "            num_params += num_params_loop\n",
    "        return num_params\n",
    "\n",
    "    def run_node(self, n:fx.Node) -> Any:\n",
    "        # Run the node\n",
    "        self.cur_macs = None\n",
    "        result = super().run_node(n)\n",
    "\n",
    "        # Retrieve the shape\n",
    "        if isinstance(result, Tensor):\n",
    "            shape = tuple(result.shape)\n",
    "        else:\n",
    "            shape = (0,0,0,0)\n",
    "        self.shapes[n] = shape\n",
    "\n",
    "        # Retrieve the module type and parameter count\n",
    "        if n.op == 'call_module':\n",
    "            submod = self.fetch_attr(n.target)\n",
    "            self.type_names[n] = submod.__class__.__name__\n",
    "            self.param_counts[n] = self.get_num_trainable_params(submod)\n",
    "            if self.cur_macs is not None: self.macs[n] = self.cur_macs\n",
    "        if n.op == 'call_function':\n",
    "            self.type_names[n] = n.target.__name__\n",
    "\n",
    "        # Return the result\n",
    "        return result\n",
    "    \n",
    "    def call_module(self, target, args, kwargs):\n",
    "        # Run the module\n",
    "        result = super().call_module(target, args, kwargs)\n",
    "\n",
    "        # Estimate the FLOPS\n",
    "        try:\n",
    "            submod = self.fetch_attr(target)\n",
    "            macs = torchprofile.profile_macs(submod, args)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f'FLOPS calculation failed for module {submod.__class__.__name__}: {e}')\n",
    "            macs = 0  \n",
    "        self.cur_macs = macs\n",
    "\n",
    "        # Return the result\n",
    "        return result\n",
    "        \n",
    "    def summary(self) -> str:\n",
    "        node_summaries : List[List[Any]] = []\n",
    "\n",
    "        for node, shape in self.shapes.items():\n",
    "            type_name = self.type_names.get(node, '')\n",
    "            num_params = self.param_counts.get(node, '')\n",
    "            macs = self.macs.get(node, '')\n",
    "            node_summaries.append(\n",
    "                [node.op, node.name, type_name, node.all_input_nodes, list(node.users.keys()), shape, num_params, macs, node.target, node.args, node.kwargs])\n",
    "\n",
    "        headers : List[str] = ['opcode', 'name', 'type', 'inputs', 'outputs', 'activations', '# params', 'MACs', 'target', 'args', 'kwargs']\n",
    "        return tabulate(node_summaries, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model: working small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistCnn(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name         type         inputs         outputs        activations       # params    MACs       target                                                      args            kwargs\n",
      "-------------  -----------  -----------  -------------  -------------  ----------------  ----------  ---------  ----------------------------------------------------------  --------------  ------------------------------------------------------------------------------------------\n",
      "placeholder    x                         []             [conv1]        (16, 1, 28, 28)                          x                                                           ()              {}\n",
      "call_module    conv1        Conv2d       [x]            [relu]         (16, 32, 26, 26)  320         3115008    conv1                                                       (x,)            {}\n",
      "call_function  relu         relu         [conv1]        [conv2]        (16, 32, 26, 26)                         <function relu at 0x7f46a59f5ea0>                           (conv1,)        {'inplace': False}\n",
      "call_module    conv2        Conv2d       [relu]         [relu_1]       (16, 64, 24, 24)  18496       169869312  conv2                                                       (relu,)         {}\n",
      "call_function  relu_1       relu         [conv2]        [max_pool2d]   (16, 64, 24, 24)                         <function relu at 0x7f46a59f5ea0>                           (conv2,)        {'inplace': False}\n",
      "call_function  max_pool2d   max_pool2d   [relu_1]       [dropout1]     (16, 64, 12, 12)                         <function boolean_dispatch.<locals>.fn at 0x7f46a59f4d30>   (relu_1, 2)     {'stride': None, 'padding': 0, 'dilation': 1, 'ceil_mode': False, 'return_indices': False}\n",
      "call_module    dropout1     Dropout      [max_pool2d]   [flatten]      (16, 64, 12, 12)  0           0          dropout1                                                    (max_pool2d,)   {}\n",
      "call_function  flatten      flatten      [dropout1]     [fc1]          (16, 9216)                               <built-in method flatten of type object at 0x7f4786e5f1c0>  (dropout1, 1)   {}\n",
      "call_module    fc1          Linear       [flatten]      [relu_2]       (16, 128)         1179776     18874368   fc1                                                         (flatten,)      {}\n",
      "call_function  relu_2       relu         [fc1]          [dropout2]     (16, 128)                                <function relu at 0x7f46a59f5ea0>                           (fc1,)          {'inplace': False}\n",
      "call_module    dropout2     Dropout      [relu_2]       [fc2]          (16, 128)         0           0          dropout2                                                    (relu_2,)       {}\n",
      "call_module    fc2          Linear       [dropout2]     [log_softmax]  (16, 10)          1290        20480      fc2                                                         (dropout2,)     {}\n",
      "call_function  log_softmax  log_softmax  [fc2]          [output]       (16, 10)                                 <function log_softmax at 0x7f46a59f6830>                    (fc2,)          {'dim': 1, '_stacklevel': 3, 'dtype': None}\n",
      "output         output                    [log_softmax]  []             (16, 10)                                 output                                                      (log_softmax,)  {}\n"
     ]
    }
   ],
   "source": [
    "interp0 = ShapeInterpreter(model0)\n",
    "interp0.run(x0)\n",
    "print(interp0.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #                   Mult-Adds                 Input Shape               Kernel Shape\n",
      "=====================================================================================================================================================================\n",
      "MnistCnn                                 [16, 10]                  --                        --                        [16, 1, 28, 28]           --\n",
      "├─Conv2d: 1-1                            [16, 32, 26, 26]          320                       3,461,120                 [16, 1, 28, 28]           [3, 3]\n",
      "├─Conv2d: 1-2                            [16, 64, 24, 24]          18,496                    170,459,136               [16, 32, 26, 26]          [3, 3]\n",
      "├─Dropout: 1-3                           [16, 64, 12, 12]          --                        --                        [16, 64, 12, 12]          --\n",
      "├─Linear: 1-4                            [16, 128]                 1,179,776                 18,876,416                [16, 9216]                --\n",
      "├─Dropout: 1-5                           [16, 128]                 --                        --                        [16, 128]                 --\n",
      "├─Linear: 1-6                            [16, 10]                  1,290                     20,640                    [16, 128]                 --\n",
      "=====================================================================================================================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 192.82\n",
      "=====================================================================================================================================================================\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 7.51\n",
      "Params size (MB): 4.80\n",
      "Estimated Total Size (MB): 12.35\n",
      "=====================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "torchinfo.summary(model0, input_size=x0.shape, verbose=1, depth=8, device=device,\n",
    "                  col_names=[\"output_size\",\"num_params\",\"mult_adds\",\"input_size\",\"kernel_size\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: working ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                   type               inputs                                 outputs                                  activations        # params    MACs       target                                                      args                                   kwargs\n",
      "-------------  ---------------------  -----------------  -------------------------------------  ---------------------------------------  -----------------  ----------  ---------  ----------------------------------------------------------  -------------------------------------  --------\n",
      "placeholder    x                                         []                                     [conv1]                                  (16, 3, 160, 160)                         x                                                           ()                                     {}\n",
      "call_module    conv1                  Conv2d             [x]                                    [bn1]                                    (16, 64, 80, 80)   9408        963379200  conv1                                                       (x,)                                   {}\n",
      "call_module    bn1                    BatchNorm2d        [conv1]                                [relu]                                   (16, 64, 80, 80)   128         6553600    bn1                                                         (conv1,)                               {}\n",
      "call_module    relu                   ReLU               [bn1]                                  [maxpool]                                (16, 64, 80, 80)   0           0          relu                                                        (bn1,)                                 {}\n",
      "call_module    maxpool                MaxPool2d          [relu]                                 [layer1_0_conv1, add]                    (16, 64, 40, 40)   0           0          maxpool                                                     (relu,)                                {}\n",
      "call_module    layer1_0_conv1         Conv2d             [maxpool]                              [layer1_0_bn1]                           (16, 64, 40, 40)   36864       943718400  layer1.0.conv1                                              (maxpool,)                             {}\n",
      "call_module    layer1_0_bn1           BatchNorm2d        [layer1_0_conv1]                       [layer1_0_relu]                          (16, 64, 40, 40)   128         1638400    layer1.0.bn1                                                (layer1_0_conv1,)                      {}\n",
      "call_module    layer1_0_relu          ReLU               [layer1_0_bn1]                         [layer1_0_conv2]                         (16, 64, 40, 40)   0           0          layer1.0.relu                                               (layer1_0_bn1,)                        {}\n",
      "call_module    layer1_0_conv2         Conv2d             [layer1_0_relu]                        [layer1_0_bn2]                           (16, 64, 40, 40)   36864       943718400  layer1.0.conv2                                              (layer1_0_relu,)                       {}\n",
      "call_module    layer1_0_bn2           BatchNorm2d        [layer1_0_conv2]                       [add]                                    (16, 64, 40, 40)   128         1638400    layer1.0.bn2                                                (layer1_0_conv2,)                      {}\n",
      "call_function  add                    add                [layer1_0_bn2, maxpool]                [layer1_0_relu_1]                        (16, 64, 40, 40)                          <built-in function add>                                     (layer1_0_bn2, maxpool)                {}\n",
      "call_module    layer1_0_relu_1        ReLU               [add]                                  [layer1_1_conv1, add_1]                  (16, 64, 40, 40)   0           0          layer1.0.relu                                               (add,)                                 {}\n",
      "call_module    layer1_1_conv1         Conv2d             [layer1_0_relu_1]                      [layer1_1_bn1]                           (16, 64, 40, 40)   36864       943718400  layer1.1.conv1                                              (layer1_0_relu_1,)                     {}\n",
      "call_module    layer1_1_bn1           BatchNorm2d        [layer1_1_conv1]                       [layer1_1_relu]                          (16, 64, 40, 40)   128         1638400    layer1.1.bn1                                                (layer1_1_conv1,)                      {}\n",
      "call_module    layer1_1_relu          ReLU               [layer1_1_bn1]                         [layer1_1_conv2]                         (16, 64, 40, 40)   0           0          layer1.1.relu                                               (layer1_1_bn1,)                        {}\n",
      "call_module    layer1_1_conv2         Conv2d             [layer1_1_relu]                        [layer1_1_bn2]                           (16, 64, 40, 40)   36864       943718400  layer1.1.conv2                                              (layer1_1_relu,)                       {}\n",
      "call_module    layer1_1_bn2           BatchNorm2d        [layer1_1_conv2]                       [add_1]                                  (16, 64, 40, 40)   128         1638400    layer1.1.bn2                                                (layer1_1_conv2,)                      {}\n",
      "call_function  add_1                  add                [layer1_1_bn2, layer1_0_relu_1]        [layer1_1_relu_1]                        (16, 64, 40, 40)                          <built-in function add>                                     (layer1_1_bn2, layer1_0_relu_1)        {}\n",
      "call_module    layer1_1_relu_1        ReLU               [add_1]                                [layer2_0_conv1, layer2_0_downsample_0]  (16, 64, 40, 40)   0           0          layer1.1.relu                                               (add_1,)                               {}\n",
      "call_module    layer2_0_conv1         Conv2d             [layer1_1_relu_1]                      [layer2_0_bn1]                           (16, 128, 20, 20)  73728       471859200  layer2.0.conv1                                              (layer1_1_relu_1,)                     {}\n",
      "call_module    layer2_0_bn1           BatchNorm2d        [layer2_0_conv1]                       [layer2_0_relu]                          (16, 128, 20, 20)  256         819200     layer2.0.bn1                                                (layer2_0_conv1,)                      {}\n",
      "call_module    layer2_0_relu          ReLU               [layer2_0_bn1]                         [layer2_0_conv2]                         (16, 128, 20, 20)  0           0          layer2.0.relu                                               (layer2_0_bn1,)                        {}\n",
      "call_module    layer2_0_conv2         Conv2d             [layer2_0_relu]                        [layer2_0_bn2]                           (16, 128, 20, 20)  147456      943718400  layer2.0.conv2                                              (layer2_0_relu,)                       {}\n",
      "call_module    layer2_0_bn2           BatchNorm2d        [layer2_0_conv2]                       [add_2]                                  (16, 128, 20, 20)  256         819200     layer2.0.bn2                                                (layer2_0_conv2,)                      {}\n",
      "call_module    layer2_0_downsample_0  Conv2d             [layer1_1_relu_1]                      [layer2_0_downsample_1]                  (16, 128, 20, 20)  8192        52428800   layer2.0.downsample.0                                       (layer1_1_relu_1,)                     {}\n",
      "call_module    layer2_0_downsample_1  BatchNorm2d        [layer2_0_downsample_0]                [add_2]                                  (16, 128, 20, 20)  256         819200     layer2.0.downsample.1                                       (layer2_0_downsample_0,)               {}\n",
      "call_function  add_2                  add                [layer2_0_bn2, layer2_0_downsample_1]  [layer2_0_relu_1]                        (16, 128, 20, 20)                         <built-in function add>                                     (layer2_0_bn2, layer2_0_downsample_1)  {}\n",
      "call_module    layer2_0_relu_1        ReLU               [add_2]                                [layer2_1_conv1, add_3]                  (16, 128, 20, 20)  0           0          layer2.0.relu                                               (add_2,)                               {}\n",
      "call_module    layer2_1_conv1         Conv2d             [layer2_0_relu_1]                      [layer2_1_bn1]                           (16, 128, 20, 20)  147456      943718400  layer2.1.conv1                                              (layer2_0_relu_1,)                     {}\n",
      "call_module    layer2_1_bn1           BatchNorm2d        [layer2_1_conv1]                       [layer2_1_relu]                          (16, 128, 20, 20)  256         819200     layer2.1.bn1                                                (layer2_1_conv1,)                      {}\n",
      "call_module    layer2_1_relu          ReLU               [layer2_1_bn1]                         [layer2_1_conv2]                         (16, 128, 20, 20)  0           0          layer2.1.relu                                               (layer2_1_bn1,)                        {}\n",
      "call_module    layer2_1_conv2         Conv2d             [layer2_1_relu]                        [layer2_1_bn2]                           (16, 128, 20, 20)  147456      943718400  layer2.1.conv2                                              (layer2_1_relu,)                       {}\n",
      "call_module    layer2_1_bn2           BatchNorm2d        [layer2_1_conv2]                       [add_3]                                  (16, 128, 20, 20)  256         819200     layer2.1.bn2                                                (layer2_1_conv2,)                      {}\n",
      "call_function  add_3                  add                [layer2_1_bn2, layer2_0_relu_1]        [layer2_1_relu_1]                        (16, 128, 20, 20)                         <built-in function add>                                     (layer2_1_bn2, layer2_0_relu_1)        {}\n",
      "call_module    layer2_1_relu_1        ReLU               [add_3]                                [layer3_0_conv1, layer3_0_downsample_0]  (16, 128, 20, 20)  0           0          layer2.1.relu                                               (add_3,)                               {}\n",
      "call_module    layer3_0_conv1         Conv2d             [layer2_1_relu_1]                      [layer3_0_bn1]                           (16, 256, 10, 10)  294912      471859200  layer3.0.conv1                                              (layer2_1_relu_1,)                     {}\n",
      "call_module    layer3_0_bn1           BatchNorm2d        [layer3_0_conv1]                       [layer3_0_relu]                          (16, 256, 10, 10)  512         409600     layer3.0.bn1                                                (layer3_0_conv1,)                      {}\n",
      "call_module    layer3_0_relu          ReLU               [layer3_0_bn1]                         [layer3_0_conv2]                         (16, 256, 10, 10)  0           0          layer3.0.relu                                               (layer3_0_bn1,)                        {}\n",
      "call_module    layer3_0_conv2         Conv2d             [layer3_0_relu]                        [layer3_0_bn2]                           (16, 256, 10, 10)  589824      943718400  layer3.0.conv2                                              (layer3_0_relu,)                       {}\n",
      "call_module    layer3_0_bn2           BatchNorm2d        [layer3_0_conv2]                       [add_4]                                  (16, 256, 10, 10)  512         409600     layer3.0.bn2                                                (layer3_0_conv2,)                      {}\n",
      "call_module    layer3_0_downsample_0  Conv2d             [layer2_1_relu_1]                      [layer3_0_downsample_1]                  (16, 256, 10, 10)  32768       52428800   layer3.0.downsample.0                                       (layer2_1_relu_1,)                     {}\n",
      "call_module    layer3_0_downsample_1  BatchNorm2d        [layer3_0_downsample_0]                [add_4]                                  (16, 256, 10, 10)  512         409600     layer3.0.downsample.1                                       (layer3_0_downsample_0,)               {}\n",
      "call_function  add_4                  add                [layer3_0_bn2, layer3_0_downsample_1]  [layer3_0_relu_1]                        (16, 256, 10, 10)                         <built-in function add>                                     (layer3_0_bn2, layer3_0_downsample_1)  {}\n",
      "call_module    layer3_0_relu_1        ReLU               [add_4]                                [layer3_1_conv1, add_5]                  (16, 256, 10, 10)  0           0          layer3.0.relu                                               (add_4,)                               {}\n",
      "call_module    layer3_1_conv1         Conv2d             [layer3_0_relu_1]                      [layer3_1_bn1]                           (16, 256, 10, 10)  589824      943718400  layer3.1.conv1                                              (layer3_0_relu_1,)                     {}\n",
      "call_module    layer3_1_bn1           BatchNorm2d        [layer3_1_conv1]                       [layer3_1_relu]                          (16, 256, 10, 10)  512         409600     layer3.1.bn1                                                (layer3_1_conv1,)                      {}\n",
      "call_module    layer3_1_relu          ReLU               [layer3_1_bn1]                         [layer3_1_conv2]                         (16, 256, 10, 10)  0           0          layer3.1.relu                                               (layer3_1_bn1,)                        {}\n",
      "call_module    layer3_1_conv2         Conv2d             [layer3_1_relu]                        [layer3_1_bn2]                           (16, 256, 10, 10)  589824      943718400  layer3.1.conv2                                              (layer3_1_relu,)                       {}\n",
      "call_module    layer3_1_bn2           BatchNorm2d        [layer3_1_conv2]                       [add_5]                                  (16, 256, 10, 10)  512         409600     layer3.1.bn2                                                (layer3_1_conv2,)                      {}\n",
      "call_function  add_5                  add                [layer3_1_bn2, layer3_0_relu_1]        [layer3_1_relu_1]                        (16, 256, 10, 10)                         <built-in function add>                                     (layer3_1_bn2, layer3_0_relu_1)        {}\n",
      "call_module    layer3_1_relu_1        ReLU               [add_5]                                [layer4_0_conv1, layer4_0_downsample_0]  (16, 256, 10, 10)  0           0          layer3.1.relu                                               (add_5,)                               {}\n",
      "call_module    layer4_0_conv1         Conv2d             [layer3_1_relu_1]                      [layer4_0_bn1]                           (16, 512, 5, 5)    1179648     471859200  layer4.0.conv1                                              (layer3_1_relu_1,)                     {}\n",
      "call_module    layer4_0_bn1           BatchNorm2d        [layer4_0_conv1]                       [layer4_0_relu]                          (16, 512, 5, 5)    1024        204800     layer4.0.bn1                                                (layer4_0_conv1,)                      {}\n",
      "call_module    layer4_0_relu          ReLU               [layer4_0_bn1]                         [layer4_0_conv2]                         (16, 512, 5, 5)    0           0          layer4.0.relu                                               (layer4_0_bn1,)                        {}\n",
      "call_module    layer4_0_conv2         Conv2d             [layer4_0_relu]                        [layer4_0_bn2]                           (16, 512, 5, 5)    2359296     943718400  layer4.0.conv2                                              (layer4_0_relu,)                       {}\n",
      "call_module    layer4_0_bn2           BatchNorm2d        [layer4_0_conv2]                       [add_6]                                  (16, 512, 5, 5)    1024        204800     layer4.0.bn2                                                (layer4_0_conv2,)                      {}\n",
      "call_module    layer4_0_downsample_0  Conv2d             [layer3_1_relu_1]                      [layer4_0_downsample_1]                  (16, 512, 5, 5)    131072      52428800   layer4.0.downsample.0                                       (layer3_1_relu_1,)                     {}\n",
      "call_module    layer4_0_downsample_1  BatchNorm2d        [layer4_0_downsample_0]                [add_6]                                  (16, 512, 5, 5)    1024        204800     layer4.0.downsample.1                                       (layer4_0_downsample_0,)               {}\n",
      "call_function  add_6                  add                [layer4_0_bn2, layer4_0_downsample_1]  [layer4_0_relu_1]                        (16, 512, 5, 5)                           <built-in function add>                                     (layer4_0_bn2, layer4_0_downsample_1)  {}\n",
      "call_module    layer4_0_relu_1        ReLU               [add_6]                                [layer4_1_conv1, add_7]                  (16, 512, 5, 5)    0           0          layer4.0.relu                                               (add_6,)                               {}\n",
      "call_module    layer4_1_conv1         Conv2d             [layer4_0_relu_1]                      [layer4_1_bn1]                           (16, 512, 5, 5)    2359296     943718400  layer4.1.conv1                                              (layer4_0_relu_1,)                     {}\n",
      "call_module    layer4_1_bn1           BatchNorm2d        [layer4_1_conv1]                       [layer4_1_relu]                          (16, 512, 5, 5)    1024        204800     layer4.1.bn1                                                (layer4_1_conv1,)                      {}\n",
      "call_module    layer4_1_relu          ReLU               [layer4_1_bn1]                         [layer4_1_conv2]                         (16, 512, 5, 5)    0           0          layer4.1.relu                                               (layer4_1_bn1,)                        {}\n",
      "call_module    layer4_1_conv2         Conv2d             [layer4_1_relu]                        [layer4_1_bn2]                           (16, 512, 5, 5)    2359296     943718400  layer4.1.conv2                                              (layer4_1_relu,)                       {}\n",
      "call_module    layer4_1_bn2           BatchNorm2d        [layer4_1_conv2]                       [add_7]                                  (16, 512, 5, 5)    1024        204800     layer4.1.bn2                                                (layer4_1_conv2,)                      {}\n",
      "call_function  add_7                  add                [layer4_1_bn2, layer4_0_relu_1]        [layer4_1_relu_1]                        (16, 512, 5, 5)                           <built-in function add>                                     (layer4_1_bn2, layer4_0_relu_1)        {}\n",
      "call_module    layer4_1_relu_1        ReLU               [add_7]                                [avgpool]                                (16, 512, 5, 5)    0           0          layer4.1.relu                                               (add_7,)                               {}\n",
      "call_module    avgpool                AdaptiveAvgPool2d  [layer4_1_relu_1]                      [flatten]                                (16, 512, 1, 1)    0           8192       avgpool                                                     (layer4_1_relu_1,)                     {}\n",
      "call_function  flatten                flatten            [avgpool]                              [fc]                                     (16, 512)                                 <built-in method flatten of type object at 0x7f4786e5f1c0>  (avgpool, 1)                           {}\n",
      "call_module    fc                     Linear             [flatten]                              [output]                                 (16, 1000)         513000      8192000    fc                                                          (flatten,)                             {}\n",
      "output         output                                    [fc]                                   []                                       (16, 1000)                                output                                                      (fc,)                                  {}\n"
     ]
    }
   ],
   "source": [
    "interp1 = ShapeInterpreter(model1)\n",
    "interp1.run(x1)\n",
    "print(interp1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #                   Mult-Adds                 Input Shape               Kernel Shape\n",
      "=====================================================================================================================================================================\n",
      "ResNet                                   [16, 1000]                --                        --                        [16, 3, 160, 160]         --\n",
      "├─Conv2d: 1-1                            [16, 64, 80, 80]          9,408                     963,379,200               [16, 3, 160, 160]         [7, 7]\n",
      "├─BatchNorm2d: 1-2                       [16, 64, 80, 80]          128                       2,048                     [16, 64, 80, 80]          --\n",
      "├─ReLU: 1-3                              [16, 64, 80, 80]          --                        --                        [16, 64, 80, 80]          --\n",
      "├─MaxPool2d: 1-4                         [16, 64, 40, 40]          --                        --                        [16, 64, 80, 80]          3\n",
      "├─Sequential: 1-5                        [16, 64, 40, 40]          --                        --                        [16, 64, 40, 40]          --\n",
      "│    └─BasicBlock: 2-1                   [16, 64, 40, 40]          --                        --                        [16, 64, 40, 40]          --\n",
      "│    │    └─Conv2d: 3-1                  [16, 64, 40, 40]          36,864                    943,718,400               [16, 64, 40, 40]          [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-2             [16, 64, 40, 40]          128                       2,048                     [16, 64, 40, 40]          --\n",
      "│    │    └─ReLU: 3-3                    [16, 64, 40, 40]          --                        --                        [16, 64, 40, 40]          --\n",
      "│    │    └─Conv2d: 3-4                  [16, 64, 40, 40]          36,864                    943,718,400               [16, 64, 40, 40]          [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-5             [16, 64, 40, 40]          128                       2,048                     [16, 64, 40, 40]          --\n",
      "│    │    └─ReLU: 3-6                    [16, 64, 40, 40]          --                        --                        [16, 64, 40, 40]          --\n",
      "│    └─BasicBlock: 2-2                   [16, 64, 40, 40]          --                        --                        [16, 64, 40, 40]          --\n",
      "│    │    └─Conv2d: 3-7                  [16, 64, 40, 40]          36,864                    943,718,400               [16, 64, 40, 40]          [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-8             [16, 64, 40, 40]          128                       2,048                     [16, 64, 40, 40]          --\n",
      "│    │    └─ReLU: 3-9                    [16, 64, 40, 40]          --                        --                        [16, 64, 40, 40]          --\n",
      "│    │    └─Conv2d: 3-10                 [16, 64, 40, 40]          36,864                    943,718,400               [16, 64, 40, 40]          [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-11            [16, 64, 40, 40]          128                       2,048                     [16, 64, 40, 40]          --\n",
      "│    │    └─ReLU: 3-12                   [16, 64, 40, 40]          --                        --                        [16, 64, 40, 40]          --\n",
      "├─Sequential: 1-6                        [16, 128, 20, 20]         --                        --                        [16, 64, 40, 40]          --\n",
      "│    └─BasicBlock: 2-3                   [16, 128, 20, 20]         --                        --                        [16, 64, 40, 40]          --\n",
      "│    │    └─Conv2d: 3-13                 [16, 128, 20, 20]         73,728                    471,859,200               [16, 64, 40, 40]          [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-14            [16, 128, 20, 20]         256                       4,096                     [16, 128, 20, 20]         --\n",
      "│    │    └─ReLU: 3-15                   [16, 128, 20, 20]         --                        --                        [16, 128, 20, 20]         --\n",
      "│    │    └─Conv2d: 3-16                 [16, 128, 20, 20]         147,456                   943,718,400               [16, 128, 20, 20]         [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-17            [16, 128, 20, 20]         256                       4,096                     [16, 128, 20, 20]         --\n",
      "│    │    └─Sequential: 3-18             [16, 128, 20, 20]         --                        --                        [16, 64, 40, 40]          --\n",
      "│    │    │    └─Conv2d: 4-1             [16, 128, 20, 20]         8,192                     52,428,800                [16, 64, 40, 40]          [1, 1]\n",
      "│    │    │    └─BatchNorm2d: 4-2        [16, 128, 20, 20]         256                       4,096                     [16, 128, 20, 20]         --\n",
      "│    │    └─ReLU: 3-19                   [16, 128, 20, 20]         --                        --                        [16, 128, 20, 20]         --\n",
      "│    └─BasicBlock: 2-4                   [16, 128, 20, 20]         --                        --                        [16, 128, 20, 20]         --\n",
      "│    │    └─Conv2d: 3-20                 [16, 128, 20, 20]         147,456                   943,718,400               [16, 128, 20, 20]         [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-21            [16, 128, 20, 20]         256                       4,096                     [16, 128, 20, 20]         --\n",
      "│    │    └─ReLU: 3-22                   [16, 128, 20, 20]         --                        --                        [16, 128, 20, 20]         --\n",
      "│    │    └─Conv2d: 3-23                 [16, 128, 20, 20]         147,456                   943,718,400               [16, 128, 20, 20]         [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-24            [16, 128, 20, 20]         256                       4,096                     [16, 128, 20, 20]         --\n",
      "│    │    └─ReLU: 3-25                   [16, 128, 20, 20]         --                        --                        [16, 128, 20, 20]         --\n",
      "├─Sequential: 1-7                        [16, 256, 10, 10]         --                        --                        [16, 128, 20, 20]         --\n",
      "│    └─BasicBlock: 2-5                   [16, 256, 10, 10]         --                        --                        [16, 128, 20, 20]         --\n",
      "│    │    └─Conv2d: 3-26                 [16, 256, 10, 10]         294,912                   471,859,200               [16, 128, 20, 20]         [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-27            [16, 256, 10, 10]         512                       8,192                     [16, 256, 10, 10]         --\n",
      "│    │    └─ReLU: 3-28                   [16, 256, 10, 10]         --                        --                        [16, 256, 10, 10]         --\n",
      "│    │    └─Conv2d: 3-29                 [16, 256, 10, 10]         589,824                   943,718,400               [16, 256, 10, 10]         [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-30            [16, 256, 10, 10]         512                       8,192                     [16, 256, 10, 10]         --\n",
      "│    │    └─Sequential: 3-31             [16, 256, 10, 10]         --                        --                        [16, 128, 20, 20]         --\n",
      "│    │    │    └─Conv2d: 4-3             [16, 256, 10, 10]         32,768                    52,428,800                [16, 128, 20, 20]         [1, 1]\n",
      "│    │    │    └─BatchNorm2d: 4-4        [16, 256, 10, 10]         512                       8,192                     [16, 256, 10, 10]         --\n",
      "│    │    └─ReLU: 3-32                   [16, 256, 10, 10]         --                        --                        [16, 256, 10, 10]         --\n",
      "│    └─BasicBlock: 2-6                   [16, 256, 10, 10]         --                        --                        [16, 256, 10, 10]         --\n",
      "│    │    └─Conv2d: 3-33                 [16, 256, 10, 10]         589,824                   943,718,400               [16, 256, 10, 10]         [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-34            [16, 256, 10, 10]         512                       8,192                     [16, 256, 10, 10]         --\n",
      "│    │    └─ReLU: 3-35                   [16, 256, 10, 10]         --                        --                        [16, 256, 10, 10]         --\n",
      "│    │    └─Conv2d: 3-36                 [16, 256, 10, 10]         589,824                   943,718,400               [16, 256, 10, 10]         [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-37            [16, 256, 10, 10]         512                       8,192                     [16, 256, 10, 10]         --\n",
      "│    │    └─ReLU: 3-38                   [16, 256, 10, 10]         --                        --                        [16, 256, 10, 10]         --\n",
      "├─Sequential: 1-8                        [16, 512, 5, 5]           --                        --                        [16, 256, 10, 10]         --\n",
      "│    └─BasicBlock: 2-7                   [16, 512, 5, 5]           --                        --                        [16, 256, 10, 10]         --\n",
      "│    │    └─Conv2d: 3-39                 [16, 512, 5, 5]           1,179,648                 471,859,200               [16, 256, 10, 10]         [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-40            [16, 512, 5, 5]           1,024                     16,384                    [16, 512, 5, 5]           --\n",
      "│    │    └─ReLU: 3-41                   [16, 512, 5, 5]           --                        --                        [16, 512, 5, 5]           --\n",
      "│    │    └─Conv2d: 3-42                 [16, 512, 5, 5]           2,359,296                 943,718,400               [16, 512, 5, 5]           [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-43            [16, 512, 5, 5]           1,024                     16,384                    [16, 512, 5, 5]           --\n",
      "│    │    └─Sequential: 3-44             [16, 512, 5, 5]           --                        --                        [16, 256, 10, 10]         --\n",
      "│    │    │    └─Conv2d: 4-5             [16, 512, 5, 5]           131,072                   52,428,800                [16, 256, 10, 10]         [1, 1]\n",
      "│    │    │    └─BatchNorm2d: 4-6        [16, 512, 5, 5]           1,024                     16,384                    [16, 512, 5, 5]           --\n",
      "│    │    └─ReLU: 3-45                   [16, 512, 5, 5]           --                        --                        [16, 512, 5, 5]           --\n",
      "│    └─BasicBlock: 2-8                   [16, 512, 5, 5]           --                        --                        [16, 512, 5, 5]           --\n",
      "│    │    └─Conv2d: 3-46                 [16, 512, 5, 5]           2,359,296                 943,718,400               [16, 512, 5, 5]           [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-47            [16, 512, 5, 5]           1,024                     16,384                    [16, 512, 5, 5]           --\n",
      "│    │    └─ReLU: 3-48                   [16, 512, 5, 5]           --                        --                        [16, 512, 5, 5]           --\n",
      "│    │    └─Conv2d: 3-49                 [16, 512, 5, 5]           2,359,296                 943,718,400               [16, 512, 5, 5]           [3, 3]\n",
      "│    │    └─BatchNorm2d: 3-50            [16, 512, 5, 5]           1,024                     16,384                    [16, 512, 5, 5]           --\n",
      "│    │    └─ReLU: 3-51                   [16, 512, 5, 5]           --                        --                        [16, 512, 5, 5]           --\n",
      "├─AdaptiveAvgPool2d: 1-9                 [16, 512, 1, 1]           --                        --                        [16, 512, 5, 5]           --\n",
      "├─Linear: 1-10                           [16, 1000]                513,000                   8,208,000                 [16, 512]                 --\n",
      "=====================================================================================================================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 14.81\n",
      "=====================================================================================================================================================================\n",
      "Input size (MB): 4.92\n",
      "Forward/backward pass size (MB): 324.53\n",
      "Params size (MB): 46.76\n",
      "Estimated Total Size (MB): 376.20\n",
      "=====================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "torchinfo.summary(model1, input_size=x1.shape, verbose=1, depth=8, device=device,\n",
    "                  col_names=[\"output_size\",\"num_params\",\"mult_adds\",\"input_size\",\"kernel_size\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to resolve an nn.Module from the `targets` string\n",
    "# From `Interpreter.fetch_attr`, called from `Interpreter.call_module`\n",
    "m = model1\n",
    "target = 'layer1.1.conv2'\n",
    "\n",
    "target_atoms = target.split('.')\n",
    "attr_itr = m\n",
    "for i, atom in enumerate(target_atoms):\n",
    "    if not hasattr(attr_itr, atom):\n",
    "        raise RuntimeError(f\"Node referenced nonexistent target {'.'.join(target_atoms[:i+1])}\")\n",
    "    attr_itr = getattr(attr_itr, atom)\n",
    "m = attr_itr\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.layer1[1].conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.conv.Conv2d"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.layer1[1].conv2.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: Broken model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x120 and 128x10)\n\nWhile executing %fc2 : [num_users=1] = call_module[target=fc2](args = (%dropout2,), kwargs = {})\nOriginal traceback:\nNone",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m interp2 \u001b[38;5;241m=\u001b[39m ShapeInterpreter(model2)\n\u001b[0;32m----> 2\u001b[0m \u001b[43minterp2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(interp2\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[0;32m~/ai/idlmav/.venv/lib/python3.10/site-packages/torch/fx/interpreter.py:146\u001b[0m, in \u001b[0;36mInterpreter.run\u001b[0;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv[node] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_traceback:\n",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m, in \u001b[0;36mShapeInterpreter.run_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, n:fx\u001b[38;5;241m.\u001b[39mNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Run the node\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_macs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Retrieve the shape\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Tensor):\n",
      "File \u001b[0;32m~/ai/idlmav/.venv/lib/python3.10/site-packages/torch/fx/interpreter.py:203\u001b[0m, in \u001b[0;36mInterpreter.run_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 68\u001b[0m, in \u001b[0;36mShapeInterpreter.call_module\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, target, args, kwargs):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Run the module\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Estimate the FLOPS\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/ai/idlmav/.venv/lib/python3.10/site-packages/torch/fx/interpreter.py:320\u001b[0m, in \u001b[0;36mInterpreter.call_module\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    318\u001b[0m submod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_attr(target)\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msubmod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai/idlmav/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai/idlmav/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ai/idlmav/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x120 and 128x10)\n\nWhile executing %fc2 : [num_users=1] = call_module[target=fc2](args = (%dropout2,), kwargs = {})\nOriginal traceback:\nNone"
     ]
    }
   ],
   "source": [
    "interp2 = ShapeInterpreter(model2)\n",
    "interp2.run(x2)\n",
    "print(interp2.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
