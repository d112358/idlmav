{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface\n",
    "Most of the code and annotations in this notebook were copied from James Reed's example [here](https://github.com/pytorch/tutorials/pull/1319) as referred to in the `torch.fx` documentation [here](https://pytorch.org/docs/stable/fx.html#examples-of-the-interpreter-pattern). A small amount of experimental code and additional annotations were added\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (beta) Building a Simple CPU Performance Profiler with FX\n",
    "**Author**: [James Reed](https://github.com/jamesr66a)  \n",
    "In this tutorial, we are going to use FX to do the following:\n",
    "1) Capture PyTorch Python code in a way that we can inspect and gather\n",
    "   statistics about the structure and execution of the code\n",
    "2) Build out a small class that will serve as a simple performance \"profiler\",\n",
    "   collecting runtime statistics about each part of the model from actual\n",
    "   runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we are going to use the torchvision ResNet18 model for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.fx\n",
    "import torchvision.models as models\n",
    "\n",
    "rn18 = models.resnet18()\n",
    "rn18.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model, we want to inspect deeper into its performance. That is, for the following invocation, which parts of the model are taking the longest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(5, 3, 224, 224)\n",
    "output = rn18(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way of answering that question is to go through the program source, add code that collects timestamps at various points in the program, and compare the difference between those timestamps to see how long the regions between the timestamps take.\n",
    "\n",
    "That technique is certainly applicable to PyTorch code, however it would be nicer if we didn't have to copy over model code and edit it, especially code we haven't written (like this torchvision model). Instead, we are going to use FX to automate this \"instrumentation\" process without needing to modify any source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get some imports out of the way (we will be using all of these later in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics, tabulate, time\n",
    "from typing import Any, Dict, List\n",
    "from torch.fx import Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: `tabulate` is an external library that is not a dependency of PyTorch. We will be using it to more easily visualize performance data. Please make sure you've installed it from your favorite Python package source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing the Model with Symbolic Tracing\n",
    "Next, we are going to use FX's symbolic tracing mechanism to capture the definition of our model in a data structure we can manipulate and examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : torch.Tensor [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %bn1 : [num_users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_module[target=relu](args = (%bn1,), kwargs = {})\n",
      "    %maxpool : [num_users=2] = call_module[target=maxpool](args = (%relu,), kwargs = {})\n",
      "    %layer1_0_conv1 : [num_users=1] = call_module[target=layer1.0.conv1](args = (%maxpool,), kwargs = {})\n",
      "    %layer1_0_bn1 : [num_users=1] = call_module[target=layer1.0.bn1](args = (%layer1_0_conv1,), kwargs = {})\n",
      "    %layer1_0_relu : [num_users=1] = call_module[target=layer1.0.relu](args = (%layer1_0_bn1,), kwargs = {})\n",
      "    %layer1_0_conv2 : [num_users=1] = call_module[target=layer1.0.conv2](args = (%layer1_0_relu,), kwargs = {})\n",
      "    %layer1_0_bn2 : [num_users=1] = call_module[target=layer1.0.bn2](args = (%layer1_0_conv2,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (%layer1_0_bn2, %maxpool), kwargs = {})\n",
      "    %layer1_0_relu_1 : [num_users=2] = call_module[target=layer1.0.relu](args = (%add,), kwargs = {})\n",
      "    %layer1_1_conv1 : [num_users=1] = call_module[target=layer1.1.conv1](args = (%layer1_0_relu_1,), kwargs = {})\n",
      "    %layer1_1_bn1 : [num_users=1] = call_module[target=layer1.1.bn1](args = (%layer1_1_conv1,), kwargs = {})\n",
      "    %layer1_1_relu : [num_users=1] = call_module[target=layer1.1.relu](args = (%layer1_1_bn1,), kwargs = {})\n",
      "    %layer1_1_conv2 : [num_users=1] = call_module[target=layer1.1.conv2](args = (%layer1_1_relu,), kwargs = {})\n",
      "    %layer1_1_bn2 : [num_users=1] = call_module[target=layer1.1.bn2](args = (%layer1_1_conv2,), kwargs = {})\n",
      "    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%layer1_1_bn2, %layer1_0_relu_1), kwargs = {})\n",
      "    %layer1_1_relu_1 : [num_users=2] = call_module[target=layer1.1.relu](args = (%add_1,), kwargs = {})\n",
      "    %layer2_0_conv1 : [num_users=1] = call_module[target=layer2.0.conv1](args = (%layer1_1_relu_1,), kwargs = {})\n",
      "    %layer2_0_bn1 : [num_users=1] = call_module[target=layer2.0.bn1](args = (%layer2_0_conv1,), kwargs = {})\n",
      "    %layer2_0_relu : [num_users=1] = call_module[target=layer2.0.relu](args = (%layer2_0_bn1,), kwargs = {})\n",
      "    %layer2_0_conv2 : [num_users=1] = call_module[target=layer2.0.conv2](args = (%layer2_0_relu,), kwargs = {})\n",
      "    %layer2_0_bn2 : [num_users=1] = call_module[target=layer2.0.bn2](args = (%layer2_0_conv2,), kwargs = {})\n",
      "    %layer2_0_downsample_0 : [num_users=1] = call_module[target=layer2.0.downsample.0](args = (%layer1_1_relu_1,), kwargs = {})\n",
      "    %layer2_0_downsample_1 : [num_users=1] = call_module[target=layer2.0.downsample.1](args = (%layer2_0_downsample_0,), kwargs = {})\n",
      "    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%layer2_0_bn2, %layer2_0_downsample_1), kwargs = {})\n",
      "    %layer2_0_relu_1 : [num_users=2] = call_module[target=layer2.0.relu](args = (%add_2,), kwargs = {})\n",
      "    %layer2_1_conv1 : [num_users=1] = call_module[target=layer2.1.conv1](args = (%layer2_0_relu_1,), kwargs = {})\n",
      "    %layer2_1_bn1 : [num_users=1] = call_module[target=layer2.1.bn1](args = (%layer2_1_conv1,), kwargs = {})\n",
      "    %layer2_1_relu : [num_users=1] = call_module[target=layer2.1.relu](args = (%layer2_1_bn1,), kwargs = {})\n",
      "    %layer2_1_conv2 : [num_users=1] = call_module[target=layer2.1.conv2](args = (%layer2_1_relu,), kwargs = {})\n",
      "    %layer2_1_bn2 : [num_users=1] = call_module[target=layer2.1.bn2](args = (%layer2_1_conv2,), kwargs = {})\n",
      "    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%layer2_1_bn2, %layer2_0_relu_1), kwargs = {})\n",
      "    %layer2_1_relu_1 : [num_users=2] = call_module[target=layer2.1.relu](args = (%add_3,), kwargs = {})\n",
      "    %layer3_0_conv1 : [num_users=1] = call_module[target=layer3.0.conv1](args = (%layer2_1_relu_1,), kwargs = {})\n",
      "    %layer3_0_bn1 : [num_users=1] = call_module[target=layer3.0.bn1](args = (%layer3_0_conv1,), kwargs = {})\n",
      "    %layer3_0_relu : [num_users=1] = call_module[target=layer3.0.relu](args = (%layer3_0_bn1,), kwargs = {})\n",
      "    %layer3_0_conv2 : [num_users=1] = call_module[target=layer3.0.conv2](args = (%layer3_0_relu,), kwargs = {})\n",
      "    %layer3_0_bn2 : [num_users=1] = call_module[target=layer3.0.bn2](args = (%layer3_0_conv2,), kwargs = {})\n",
      "    %layer3_0_downsample_0 : [num_users=1] = call_module[target=layer3.0.downsample.0](args = (%layer2_1_relu_1,), kwargs = {})\n",
      "    %layer3_0_downsample_1 : [num_users=1] = call_module[target=layer3.0.downsample.1](args = (%layer3_0_downsample_0,), kwargs = {})\n",
      "    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%layer3_0_bn2, %layer3_0_downsample_1), kwargs = {})\n",
      "    %layer3_0_relu_1 : [num_users=2] = call_module[target=layer3.0.relu](args = (%add_4,), kwargs = {})\n",
      "    %layer3_1_conv1 : [num_users=1] = call_module[target=layer3.1.conv1](args = (%layer3_0_relu_1,), kwargs = {})\n",
      "    %layer3_1_bn1 : [num_users=1] = call_module[target=layer3.1.bn1](args = (%layer3_1_conv1,), kwargs = {})\n",
      "    %layer3_1_relu : [num_users=1] = call_module[target=layer3.1.relu](args = (%layer3_1_bn1,), kwargs = {})\n",
      "    %layer3_1_conv2 : [num_users=1] = call_module[target=layer3.1.conv2](args = (%layer3_1_relu,), kwargs = {})\n",
      "    %layer3_1_bn2 : [num_users=1] = call_module[target=layer3.1.bn2](args = (%layer3_1_conv2,), kwargs = {})\n",
      "    %add_5 : [num_users=1] = call_function[target=operator.add](args = (%layer3_1_bn2, %layer3_0_relu_1), kwargs = {})\n",
      "    %layer3_1_relu_1 : [num_users=2] = call_module[target=layer3.1.relu](args = (%add_5,), kwargs = {})\n",
      "    %layer4_0_conv1 : [num_users=1] = call_module[target=layer4.0.conv1](args = (%layer3_1_relu_1,), kwargs = {})\n",
      "    %layer4_0_bn1 : [num_users=1] = call_module[target=layer4.0.bn1](args = (%layer4_0_conv1,), kwargs = {})\n",
      "    %layer4_0_relu : [num_users=1] = call_module[target=layer4.0.relu](args = (%layer4_0_bn1,), kwargs = {})\n",
      "    %layer4_0_conv2 : [num_users=1] = call_module[target=layer4.0.conv2](args = (%layer4_0_relu,), kwargs = {})\n",
      "    %layer4_0_bn2 : [num_users=1] = call_module[target=layer4.0.bn2](args = (%layer4_0_conv2,), kwargs = {})\n",
      "    %layer4_0_downsample_0 : [num_users=1] = call_module[target=layer4.0.downsample.0](args = (%layer3_1_relu_1,), kwargs = {})\n",
      "    %layer4_0_downsample_1 : [num_users=1] = call_module[target=layer4.0.downsample.1](args = (%layer4_0_downsample_0,), kwargs = {})\n",
      "    %add_6 : [num_users=1] = call_function[target=operator.add](args = (%layer4_0_bn2, %layer4_0_downsample_1), kwargs = {})\n",
      "    %layer4_0_relu_1 : [num_users=2] = call_module[target=layer4.0.relu](args = (%add_6,), kwargs = {})\n",
      "    %layer4_1_conv1 : [num_users=1] = call_module[target=layer4.1.conv1](args = (%layer4_0_relu_1,), kwargs = {})\n",
      "    %layer4_1_bn1 : [num_users=1] = call_module[target=layer4.1.bn1](args = (%layer4_1_conv1,), kwargs = {})\n",
      "    %layer4_1_relu : [num_users=1] = call_module[target=layer4.1.relu](args = (%layer4_1_bn1,), kwargs = {})\n",
      "    %layer4_1_conv2 : [num_users=1] = call_module[target=layer4.1.conv2](args = (%layer4_1_relu,), kwargs = {})\n",
      "    %layer4_1_bn2 : [num_users=1] = call_module[target=layer4.1.bn2](args = (%layer4_1_conv2,), kwargs = {})\n",
      "    %add_7 : [num_users=1] = call_function[target=operator.add](args = (%layer4_1_bn2, %layer4_0_relu_1), kwargs = {})\n",
      "    %layer4_1_relu_1 : [num_users=1] = call_module[target=layer4.1.relu](args = (%add_7,), kwargs = {})\n",
      "    %avgpool : [num_users=1] = call_module[target=avgpool](args = (%layer4_1_relu_1,), kwargs = {})\n",
      "    %flatten : [num_users=1] = call_function[target=torch.flatten](args = (%avgpool, 1), kwargs = {})\n",
      "    %fc : [num_users=1] = call_module[target=fc](args = (%flatten,), kwargs = {})\n",
      "    return fc\n"
     ]
    }
   ],
   "source": [
    "traced_rn18 = torch.fx.symbolic_trace(rn18)\n",
    "print(traced_rn18.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a Graph representation of the ResNet18 model. A Graph consists of a series of Nodes connected to each other. Each Node represents a call-site in the Python code (whether to a function, a module, or a method) and the edges (represented as `args` and `kwargs` on each node) represent the values passed between these call-sites. More information about the Graph representation and the rest of FX's APIs ca be found at the [FX documentation](https://pytorch.org/docs/master/fx.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Profiling Interpreter \n",
    "Next, we are going to create a class that inherits from `torch.fx.Interpreter`. Though the `GraphModule` that `symbolic_trace` produces compiles Python code that is run when you call a `GraphModule`, an alternative way to run a `GraphModule` is by executing each `Node` in the `Graph` one by one. That is the functionality that `Interpreter` provides: It interprets the graph node- by-node. \n",
    "\n",
    "By inheriting from `Interpreter`, we can override various functionality and install the profiling behavior we want. The goal is to have an object to which we can pass a model, invoke the model 1 or more times, then get statistics about how long the model and each part of the model took during those runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional note:\n",
    "* The original source, which was not in notebook format, has prose in-between functions of the same class\n",
    "* Here I've copied the interleaved code and prose bits in markdown cells and added only the code at the end in an executable cell\n",
    "* I've specified the Python language for the code markdown cells and it renders beautifully in VSCode's Jupyter notebook, I hope it does the same for whoever reads it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our `ProfilingInterpreter` class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "class ProfilingInterpreter(Interpreter):\n",
    "    def __init__(self, mod : torch.nn.Module):\n",
    "        # Rather than have the user symbolically trace their model,\n",
    "        # we're going to do it in the constructor. As a result, the\n",
    "        # user can pass in any ``Module`` without having to worry about\n",
    "        # symbolic tracing APIs\n",
    "        gm = torch.fx.symbolic_trace(mod)\n",
    "        super().__init__(gm)\n",
    "\n",
    "        # We are going to store away two things here:\n",
    "        #\n",
    "        # 1. A list of total runtimes for ``mod``. In other words, we are\n",
    "        #    storing away the time ``mod(...)`` took each time this\n",
    "        #    interpreter is called.\n",
    "        self.total_runtime_sec : List[float] = []\n",
    "        # 2. A map from ``Node`` to a list of times (in seconds) that\n",
    "        #    node took to run. This can be seen as similar to (1) but\n",
    "        #    for specific sub-parts of the model.\n",
    "        self.runtimes_sec : Dict[torch.fx.Node, List[float]] = {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's override our first method: `run()`. `Interpreter`'s `run` method is the top-level entrypoint for execution of the model. We will want to intercept this so that we can record the total runtime of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def run(self, *args) -> Any:\n",
    "        # Record the time we started running the model\n",
    "        t_start = time.time()\n",
    "        # Run the model by delegating back into Interpreter.run()\n",
    "        return_val = super().run(*args)\n",
    "        # Record the time we finished running the model\n",
    "        t_end = time.time()\n",
    "        # Store the total elapsed time this model execution took in the\n",
    "        # ProfilingInterpreter\n",
    "        self.total_runtime_sec.append(t_end - t_start)\n",
    "        return return_val\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are going to define a method (one which doesn't override any ``Interpreter`` method) that provides us a nice, organized view of the data we have collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def summary(self, should_sort : bool = False) -> str:\n",
    "        # Build up a list of summary information for each node\n",
    "        node_summaries : List[List[Any]] = []\n",
    "        # Calculate the mean runtime for the whole network. Because the\n",
    "        # network may have been called multiple times during profiling,\n",
    "        # we need to summarize the runtimes. We choose to use the\n",
    "        # arithmetic mean for this.\n",
    "        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n",
    "\n",
    "        # For each node, record summary statistics\n",
    "        for node, runtimes in self.runtimes_sec.items():\n",
    "            # Similarly, compute the mean runtime for ``node``\n",
    "            mean_runtime = statistics.mean(runtimes)\n",
    "            # For easier understanding, we also compute the percentage\n",
    "            # time each node took with respect to the whole network.\n",
    "            pct_total = mean_runtime / mean_total_runtime * 100\n",
    "            # Record the node's type, name of the node, mean runtime, and\n",
    "            # percent runtim\n",
    "            node_summaries.append(\n",
    "                [node.op, str(node), mean_runtime, pct_total])\n",
    "\n",
    "        # One of the most important questions to answer when doing performance\n",
    "        # profiling is \"Which op(s) took the longest?\". We can make this easy\n",
    "        # to see by providing sorting functionality in our summary view\n",
    "        if should_sort:\n",
    "            node_summaries.sort(key=lambda s: s[2], reverse=True)\n",
    "\n",
    "        # Use the ``tabulate`` library to create a well-formatted table\n",
    "        # presenting our summary information\n",
    "        headers : List[str] = [\n",
    "            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n",
    "        ]\n",
    "        return tabulate.tabulate(node_summaries, headers=headers)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfilingInterpreter(Interpreter):\n",
    "    def __init__(self, mod : torch.nn.Module):\n",
    "        gm = torch.fx.symbolic_trace(mod)\n",
    "        super().__init__(gm)\n",
    "\n",
    "        self.total_runtime_sec : List[float] = []\n",
    "        self.runtimes_sec : Dict[torch.fx.Node, List[float]] = {}\n",
    "\n",
    "    def run(self, *args) -> Any:\n",
    "        t_start = time.time()\n",
    "        return_val = super().run(*args)\n",
    "        t_end = time.time()\n",
    "        self.total_runtime_sec.append(t_end - t_start)\n",
    "        return return_val\n",
    "    \n",
    "    \n",
    "    def run_node(self, n : torch.fx.Node) -> Any:\n",
    "        t_start = time.time()\n",
    "        return_val = super().run_node(n)\n",
    "        t_end = time.time()\n",
    "        self.runtimes_sec.setdefault(n, [])\n",
    "        self.runtimes_sec[n].append(t_end - t_start)        \n",
    "        return return_val\n",
    "        \n",
    "    def summary(self, should_sort : bool = False) -> str:\n",
    "        node_summaries : List[List[Any]] = []\n",
    "        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n",
    "\n",
    "        for node, runtimes in self.runtimes_sec.items():\n",
    "            mean_runtime = statistics.mean(runtimes)\n",
    "            pct_total = mean_runtime / mean_total_runtime * 100\n",
    "            node_summaries.append(\n",
    "                [node.op, str(node), mean_runtime, pct_total])\n",
    "\n",
    "        if should_sort:\n",
    "            node_summaries.sort(key=lambda s: s[2], reverse=True)\n",
    "\n",
    "        headers : List[str] = [\n",
    "            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n",
    "        ]\n",
    "        return tabulate.tabulate(node_summaries, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We use Python's `time.time` function to pull wall clock timestamps and compare them. This is not the most accurate way to measure performance, and will only give us a first- order approximation. We use this simple technique only for the purpose of demonstration in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the Performance of ResNet18\n",
    "We can now use ``ProfilingInterpreter`` to inspect the performance characteristics of our ResNet18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Op type        Op                       Average runtime (s)    Pct total runtime\n",
      "-------------  ---------------------  ---------------------  -------------------\n",
      "call_module    conv1                            0.00769258             7.32355\n",
      "call_module    maxpool                          0.00708365             6.74384\n",
      "call_module    layer4_1_conv2                   0.00605249             5.76215\n",
      "call_module    layer4_1_conv1                   0.00582862             5.54901\n",
      "call_module    layer1_0_conv1                   0.00530171             5.04738\n",
      "call_module    layer4_0_conv2                   0.00476933             4.54053\n",
      "call_module    layer1_1_conv2                   0.00465536             4.43204\n",
      "call_module    layer3_1_conv2                   0.00450635             4.29017\n",
      "call_module    layer2_0_conv1                   0.00436521             4.1558\n",
      "call_module    layer2_1_conv2                   0.00431895             4.11177\n",
      "call_module    layer1_0_conv2                   0.00422049             4.01802\n",
      "call_module    layer2_0_conv2                   0.00402045             3.82759\n",
      "call_module    layer4_0_conv1                   0.00370479             3.52706\n",
      "call_module    layer1_1_conv1                   0.00369692             3.51957\n",
      "call_module    layer3_1_conv1                   0.00368261             3.50595\n",
      "call_module    layer3_0_conv2                   0.00332808             3.16843\n",
      "call_module    layer2_1_conv1                   0.00331712             3.15799\n",
      "call_module    bn1                              0.002707               2.57715\n",
      "call_module    layer2_0_downsample_0            0.00264335             2.51654\n",
      "call_module    layer3_0_conv1                   0.00213528             2.03284\n",
      "call_module    relu                             0.00189137             1.80064\n",
      "call_function  add_1                            0.000903845            0.860486\n",
      "call_module    layer4_0_downsample_0            0.000885725            0.843235\n",
      "call_module    layer1_1_bn1                     0.000838757            0.79852\n",
      "call_module    layer3_0_downsample_0            0.000697136            0.663693\n",
      "call_function  add                              0.000694036            0.660742\n",
      "call_module    layer2_0_bn1                     0.000623941            0.59401\n",
      "call_module    layer1_0_bn1                     0.000605106            0.576078\n",
      "call_module    layer1_1_bn2                     0.000599623            0.570858\n",
      "call_module    layer2_0_downsample_1            0.00057292             0.545436\n",
      "call_module    layer1_0_bn2                     0.000531673            0.506168\n",
      "call_module    layer3_1_bn2                     0.000379562            0.361354\n",
      "call_module    layer2_0_bn2                     0.000372171            0.354318\n",
      "call_function  add_2                            0.000345945            0.32935\n",
      "call_module    layer1_1_relu_1                  0.000341415            0.325037\n",
      "call_module    layer4_0_bn2                     0.00031352             0.29848\n",
      "call_module    layer1_1_relu                    0.000307083            0.292352\n",
      "call_module    layer2_1_bn2                     0.000292063            0.278052\n",
      "call_function  add_3                            0.000289917            0.276009\n",
      "call_module    layer4_1_bn2                     0.000274181            0.261028\n",
      "call_module    fc                               0.00026989             0.256943\n",
      "call_module    avgpool                          0.000256538            0.244232\n",
      "call_module    layer3_0_bn2                     0.000220299            0.209731\n",
      "call_module    layer2_0_relu                    0.000208139            0.198155\n",
      "call_module    layer1_0_relu_1                  0.000207663            0.197701\n",
      "call_module    layer4_1_bn1                     0.000202417            0.192707\n",
      "call_module    layer2_1_bn1                     0.000192404            0.183174\n",
      "call_module    layer3_1_bn1                     0.000191927            0.18272\n",
      "call_module    layer4_0_bn1                     0.000178576            0.170009\n",
      "call_module    layer4_0_downsample_1            0.000177383            0.168874\n",
      "call_module    layer4_0_relu_1                  0.000176668            0.168193\n",
      "call_module    layer1_0_relu                    0.000174522            0.16615\n",
      "call_function  add_5                            0.000162363            0.154574\n",
      "call_module    layer4_0_relu                    0.000162125            0.154347\n",
      "call_module    layer3_0_downsample_1            0.000145912            0.138913\n",
      "call_module    layer2_0_relu_1                  0.000124216            0.118257\n",
      "call_module    layer3_0_bn1                     0.000119925            0.114172\n",
      "call_module    layer3_1_relu_1                  0.000116587            0.110994\n",
      "call_function  add_4                            0.000112057            0.106681\n",
      "call_module    layer2_1_relu_1                  0.000104189            0.0991908\n",
      "call_function  add_6                            0.000101805            0.096921\n",
      "call_module    layer4_1_relu                    9.75132e-05            0.0928353\n",
      "call_module    layer3_1_relu                    9.25064e-05            0.0880687\n",
      "call_function  add_7                            8.58307e-05            0.0817133\n",
      "call_module    layer3_0_relu_1                  7.77245e-05            0.0739959\n",
      "call_module    layer2_1_relu                    7.7486e-05             0.0737689\n",
      "call_module    layer4_1_relu_1                  7.51019e-05            0.0714991\n",
      "call_function  flatten                          6.67572e-05            0.0635548\n",
      "call_module    layer3_0_relu                    6.22272e-05            0.0592421\n",
      "placeholder    x                                2.47955e-05            0.0236061\n",
      "output         output                           1.66893e-05            0.0158887\n"
     ]
    }
   ],
   "source": [
    "interp = ProfilingInterpreter(rn18)\n",
    "interp.run(input)\n",
    "print(interp.summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things we should call out here: \n",
    "* MaxPool2d takes up the most time. This is a known issue: https://github.com/pytorch/pytorch/issues/51393\n",
    "* BatchNorm2d also takes up significant time. We can continue this line of thinking and optimize this in the Conv-BN Fusion with FX tutorial TODO: link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional note:\n",
    "* When I executed this in December 2024, the above observations were no longer true and the `conv` modules seemed to dominate the runtime, as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As we can see, using FX we can easily capture PyTorch programs (even ones we don't have the source code for!) in a machine-interpretable format and use that for analysis, such as the performance analysis we've done here. FX opens up an exiciting world of possibilities for working with PyTorch programs.\n",
    "\n",
    "Finally, since FX is still in beta, we would be happy to hear any feedback you have about using it. Please feel free to use the PyTorch Forums (https://discuss.pytorch.org/) and the issue tracker (https://github.com/pytorch/pytorch/issues) to provide any feedback you might have."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
